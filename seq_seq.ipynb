{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "au9Mz5iEWniQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Generate dummy data\n",
        "def generate_data(num_samples, seq_len, vocab_size):\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        src = [random.randint(1, vocab_size-1) for _ in range(seq_len)]\n",
        "        tgt = src[::-1]  # Reverse the source sequence for the target\n",
        "        data.append((src, tgt))\n",
        "    return data\n",
        "\n",
        "vocab_size = 20\n",
        "seq_len = 20\n",
        "num_samples = 10000\n",
        "data = generate_data(num_samples, seq_len, vocab_size)\n",
        "\n",
        "# Define dummy vocab (for demonstration purposes)\n",
        "vocab = {str(i): i for i in range(vocab_size)}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Seq2SeqDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.data[idx]\n",
        "        return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
        "\n",
        "dataset = Seq2SeqDataset(data)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "4fJcZ0iqWomG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        max_len = encoder_outputs.size(1)\n",
        "        H = hidden.repeat(max_len, 1, 1).transpose(0, 1)\n",
        "        attn_energies = self.score(H, encoder_outputs)\n",
        "        return torch.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "\n",
        "    def score(self, hidden, encoder_outputs):\n",
        "        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
        "        energy = energy.transpose(1, 2)\n",
        "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
        "        energy = torch.bmm(v, energy)\n",
        "        return energy.squeeze(1)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, hidden_size, num_layers):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.embedding(x)\n",
        "        outputs, (hidden, cell) = self.lstm(embedding)\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, embed_size, hidden_size, num_layers, attention):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_size, embed_size)\n",
        "        self.lstm = nn.LSTM(hidden_size + embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell, encoder_outputs):\n",
        "        x = x.unsqueeze(1)\n",
        "        embedding = self.embedding(x)\n",
        "        attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs)\n",
        "        rnn_input = torch.cat((embedding, context), dim=2)\n",
        "        outputs, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
        "        predictions = self.fc(outputs.squeeze(1))\n",
        "        return predictions, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source.shape[0]\n",
        "        target_len = target.shape[1]\n",
        "        target_vocab_size = self.decoder.output_size\n",
        "\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(device)\n",
        "\n",
        "        encoder_outputs, hidden, cell = self.encoder(source)\n",
        "\n",
        "        x = target[:, 0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell, encoder_outputs)\n",
        "            outputs[:, t, :] = output\n",
        "            best_guess = output.argmax(1)\n",
        "            x = target[:, t] if random.random() < teacher_forcing_ratio else best_guess\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Initialize the model\n",
        "input_size = vocab_size\n",
        "output_size = vocab_size\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 2\n",
        "\n",
        "attention = Attention(hidden_size)\n",
        "encoder = Encoder(input_size, embed_size, hidden_size, num_layers).to(device)\n",
        "decoder = Decoder(output_size, embed_size, hidden_size, num_layers, attention).to(device)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "9oNi2uWQWq-7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (src, tgt) in enumerate(dataloader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "\n",
        "        output = output[:, 1:].reshape(-1, output.shape[2])\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "RBdNJcjYWtsc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c1da11e-d5d8-4155-a1c4-bc7bfb3775be"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.0653\n",
            "Epoch [2/20], Loss: 0.0114\n",
            "Epoch [3/20], Loss: 0.0018\n",
            "Epoch [4/20], Loss: 0.0069\n",
            "Epoch [5/20], Loss: 0.0004\n",
            "Epoch [6/20], Loss: 0.0002\n",
            "Epoch [7/20], Loss: 0.0001\n",
            "Epoch [8/20], Loss: 0.0001\n",
            "Epoch [9/20], Loss: 0.0001\n",
            "Epoch [10/20], Loss: 0.0000\n",
            "Epoch [11/20], Loss: 0.0000\n",
            "Epoch [12/20], Loss: 0.0000\n",
            "Epoch [13/20], Loss: 0.0000\n",
            "Epoch [14/20], Loss: 0.0000\n",
            "Epoch [15/20], Loss: 0.0000\n",
            "Epoch [16/20], Loss: 0.0000\n",
            "Epoch [17/20], Loss: 0.0000\n",
            "Epoch [18/20], Loss: 0.0000\n",
            "Epoch [19/20], Loss: 0.0000\n",
            "Epoch [20/20], Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (src, tgt) in enumerate(dataloader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        output = model(src, tgt, teacher_forcing_ratio=0)\n",
        "        output = output[:, 1:].reshape(-1, output.shape[2])\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "        pred = output.argmax(1).view(-1, seq_len-1)\n",
        "        print(f'Source: {src[0].cpu().numpy()}')\n",
        "        print(f'Target: {tgt.view(-1, seq_len-1)[0].cpu().numpy()}')\n",
        "        print(f'Predicted: {pred[0].cpu().numpy()}')\n",
        "        break\n"
      ],
      "metadata": {
        "id": "DRkv8hNPWwl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1c8ec9-7a6b-443e-d681-42acd90e25c4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: [ 5  3 17  5  6 18 10  5  9 10 17 19 12 10  3 16 13 16 10 13]\n",
            "Target: [10 16 13 16  3 10 12 19 17 10  9  5 10 18  6  5 17  3  5]\n",
            "Predicted: [10 16 13 16  3 10 12 19 17 10  9  5 10 18  6  5 17  3  5]\n"
          ]
        }
      ]
    }
  ]
}